<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="Multi-View Attribute Graph Convolution Networks for Clustering, java,安全,python"><meta name="description" content="一个小白成长史"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>Multi-View Attribute Graph Convolution Networks for Clustering | FSRM</title><link rel="icon" type="image/png" href="/favicon.png"><link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="/css/matery.css"><link rel="stylesheet" type="text/css" href="/css/my.css"><script src="/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="FSRM" type="application/atom+xml"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">FSRM</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>归档</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/friends" class="waves-effect waves-light"><i class="fas fa-address-book" style="zoom:.6"></i> <span>友情链接</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">FSRM</div><div class="logo-desc">一个小白成长史</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="/friends" class="waves-effect waves-light"><i class="fa-fw fas fa-address-book"></i> 友情链接</a></li><li><div class="divider"></div></li><li><a href="https://github.com/ghyjn0" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i>Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/ghyjn0" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url(/medias/featureimages/7.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">Multi-View Attribute Graph Convolution Networks for Clustering</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/%E8%AE%BA%E6%96%87/"><span class="chip bg-color">论文</span> </a><a href="/tags/%E6%B7%B1%E5%BA%A6%E8%81%9A%E7%B1%BB/"><span class="chip bg-color">深度聚类</span> </a><a href="/tags/%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">图深度学习</span> </a><a href="/tags/%E5%A4%9A%E8%A7%86%E5%9B%BE/"><span class="chip bg-color">多视图</span></a></div></div><div class="col s5 right-align"></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2023-03-26</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2023-03-28</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.7k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 9 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="论文阅读04-Multi-View-Attribute-Graph-Convolution-Networks-for-Clustering：MAGCN"><a href="#论文阅读04-Multi-View-Attribute-Graph-Convolution-Networks-for-Clustering：MAGCN" class="headerlink" title="论文阅读04-Multi-View Attribute Graph Convolution Networks for Clustering：MAGCN"></a>论文阅读04-Multi-View Attribute Graph Convolution Networks for Clustering：MAGCN</h1><h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><p>论文地址：<a target="_blank" rel="noopener" href="https://www.ijcai.org/proceedings/2020/411">Multi-View Attribute Graph Convolution Networks for Clustering | IJCAI</a></p><p>论文代码：<a target="_blank" rel="noopener" href="https://github.com/IMKBLE/MAGCN">MAGCN</a></p><h2 id="1-多视图属性聚类：MAGCN"><a href="#1-多视图属性聚类：MAGCN" class="headerlink" title="1.多视图属性聚类：MAGCN"></a><strong>1.多视图属性聚类：MAGCN</strong></h2><h3 id="1-存在问题：GNN-融入Multi-View-Graph"><a href="#1-存在问题：GNN-融入Multi-View-Graph" class="headerlink" title="1.存在问题：GNN 融入Multi-View Graph"></a>1.存在问题：GNN 融入Multi-View Graph</h3><p>1）他们<strong>不能将指定学习的不同权重的分配给邻域内的不同节点；</strong></p><p>2）他们可能<strong>忽略了进行节点属性和图结构的重构</strong>以提高鲁棒性；</p><p>3）对于<strong>不同视图之间的一致性关系</strong>，<strong>没有明确考虑相似距离度量</strong>。</p><h3 id="2-解决问题：MAGCN"><a href="#2-解决问题：MAGCN" class="headerlink" title="2.解决问题：MAGCN"></a>2.解决问题：MAGCN</h3><p>本论文提出了一种新的<strong>多视图属性图卷积网络</strong>，用于<strong>聚类（MAGCN）多视图属性的图结构数据</strong></p><ol><li><p>为了将可<code>学习的权重分配给不同的节点</code>，MAGCN开发了<code>具有注意机制的多视图属性图卷积编码器</code>，用于<code>从多视图图数据中学习图嵌入</code>。</p></li><li><p><code>属性和图重建均由 MAGCN 的图卷积解码器</code>计算。</p></li><li><p>将<code>多视图图数据之间</code>的<code>几何关系和概率分布一致性</code>纳入<code>MAGCN的一致嵌入编码器中</code>，以进一步促进聚类任务。</p></li></ol><p><code>编码器1：</code></p><p><strong>开发多视图属性图注意力网络</strong>以<strong>减少噪声/冗余并学习多视图图数据的图嵌入特征。</strong></p><p><code>编码器2：</code></p><p><strong>开发一致的嵌入编码器来</strong>捕获<strong>不同视图之间的几何关系和概率分布的一致性，</strong>从而<strong>自适应地为多视图属性找到一致的聚类嵌入空间</strong>。</p><h2 id="2-MAGCN模型创新点"><a href="#2-MAGCN模型创新点" class="headerlink" title="2.MAGCN模型创新点"></a>2.MAGCN模型创新点</h2><ol><li>我们提出了一种<strong>新的多视图属性图卷积网络</strong>，用于对<strong>多视图属性的图结构数据进行聚类</strong>。：<code>为不同的邻域节点分配不同的权重</code></li><li>我们开发了<strong>具有注意机制的多视图属性图卷积编码器</strong>，以<strong>减少多视图图数据的噪声/冗余</strong>。</li><li>此外，还考虑了<strong>节点属性和图结构的重构以提高鲁棒性</strong>。:<code>重构节点信息和结构信息提高模型鲁棒性</code><strong>一致性嵌入编码器旨在通过探索不同视图的几何关系和概率分布一致性来提取多个视图之间的一致性信息</strong>。：<code>多视图聚类特点应用</code></li></ol><h2 id="3-MAGCN-模型"><a href="#3-MAGCN-模型" class="headerlink" title="3.MAGCN 模型"></a>3.MAGCN 模型</h2><h3 id="1-MAGCN-先验知识"><a href="#1-MAGCN-先验知识" class="headerlink" title="1.MAGCN 先验知识"></a>1.MAGCN 先验知识</h3><blockquote><ol><li>MAGCN的编码器</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303271434749.png" alt="image-20230327143439693"></p><p>这个X 特征重构中GAT来自于：[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.10715">1905.10715] Graph Attention Auto-Encoders (arxiv.org)</a></p><blockquote><ol start="2"><li>GAT 和GATE 的区别</li></ol></blockquote><p>GATE:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.10715">1905.10715] Graph Attention Auto-Encoders (arxiv.org)</a></p><p>GAT: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10903">1710.10903] Graph Attention Networks (arxiv.org)</a></p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303280939682.png" alt="image-20230327200539965"></p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303280938317.png" alt="image-20230327200624428"></p><blockquote><ol start="3"><li>GATE 和GAT 区别</li></ol></blockquote><p>GATE:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.10715">1905.10715] Graph Attention Auto-Encoders (arxiv.org)</a></p><p>GAT: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10903">1710.10903] Graph Attention Networks (arxiv.org)</a></p><p><code>GATE</code> 的主要贡献是<code>逆转编码过程</code>，以便<code>在没有任何监督的情况下学习节点表示</code>。为此，我们<code>使用与编码器层数相同的解码器。每个解码器层都试图反转其相应编码器层的过程</code></p><p><strong>GATE 中编码器</strong>：使用的就是GAT 的共享参数机制，几乎是一样。</p><p>区别一激活函数:</p><p>​ <strong>GATE中采用的sigmoid的激活函数，GAT中采用的是LeajyReLU的函数</strong>，</p><p>区别二 激活函数的位置：</p><p><strong>GATE和GAT激活函数在层级传播公式，激活顺序不一样</strong></p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281001261.png" alt="image-20230328100134173"></p><p>GATE中的解码器：</p><p>每个解码器层根据<code>节点的相关性</code>利用其<code>邻居的表示来重建节点的表示</code></p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281249004.png" alt="image-20230328124945950"></p><p>简化公式</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281258526.png" alt="image-20230328125848446"></p><blockquote><ol start="4"><li>GATE 步骤流程</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281302765.png" alt="image-20230328130226722"></p><blockquote><ol start="5"><li>图中参数</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303271437321.png" alt="image-20230327143739274"></p><h3 id="2-模型架构图"><a href="#2-模型架构图" class="headerlink" title="2.模型架构图"></a>2.模型架构图</h3><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303271445814.png" alt="image-20230327144517730"></p><p><code>MAGCN模型主要有两个编码器</code>组成：<strong>多视图属性图卷积编码器</strong>和<strong>一致嵌入编码器</strong></p><p>用于聚类的多视图属性图卷积网络 (MAGCN) 的框架。</p><ol><li>具有注意机制的多视图属性图卷积编码器：它们用于<code>从节点属性和图数据（图结构）中学习图嵌入</code>。<code>执行属性和图形重建以进行端到端学习</code>。</li></ol><ol start="2"><li>一致嵌入编码器(Consistent embedding encoders)：通过<code>几何关系和概率分布的一致性</code>，<code>进一步在多个视图之间获得一致的聚类嵌入</code>。</li></ol><p><strong>MAGCN 模型训练过程：</strong></p><p>我们首先将多视图图数据 Xm，通过多视图属性图卷积编码器 编码为图嵌入。将 Hm 馈入一致的嵌入编码器并获得一致的聚类嵌入 Z。<code>聚类过程最终在由 Z 计算出的理想嵌入内在描述空间上进行</code>。</p><h3 id="3-MAGCN-模块详细"><a href="#3-MAGCN-模块详细" class="headerlink" title="3. MAGCN 模块详细"></a>3. MAGCN 模块详细</h3><h4 id="1-多视图属性图卷积编码器：Multi-view-Attribute-Graph-Convolution-Encoder"><a href="#1-多视图属性图卷积编码器：Multi-view-Attribute-Graph-Convolution-Encoder" class="headerlink" title="1. 多视图属性图卷积编码器：Multi-view Attribute Graph Convolution Encoder"></a>1. 多视图属性图卷积编码器：Multi-view Attribute Graph Convolution Encoder</h4><blockquote><ol><li>主要作用</li></ol></blockquote><p>在多视图属性图卷积编码器中，第一个编码器将多视图节点属性矩阵和图结构映射到图嵌入空间。</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303271434749.png" alt="image-20230327143439693"></p><blockquote><ol start="2"><li>编码器：encoding</li></ol></blockquote><p>为了更好的为<code>自身节点和邻域节点分配可学习的权重</code>，在节点之间使<code>用了带有共享参数的注意力机制：GAT</code>。：采用GCN+Attention的机制对各个view下的特征信息进行聚合，并得到各自的embedding</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281349815.png" alt="image-20230328134953738"></p><p><strong>在每一次用GCN进行特征聚合之前，先要按照这个attention机制求得attention矩阵，然后将其作为G GG输入到GCN框架中</strong>.</p><blockquote><ol start="3"><li>解码器：decoding</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281546490.png" alt="image-20230328154629420"></p><blockquote><ol start="4"><li>损失函数</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281413862.png" alt="image-20230328141305813"></p><p>比较<code>decoding的结果与最初的X 是否逼近（这是指特征信息的逼近），G 的逼近</code>，越逼近说明网络中encoding和decoding过程中没有损失太多的信息</p><p><code>M :表示视图的个数，因此需要把每一个视图重构出来的损失相加和</code></p><h4 id="2-一致嵌入编码器：Consistent-embedding-encoders"><a href="#2-一致嵌入编码器：Consistent-embedding-encoders" class="headerlink" title="2.一致嵌入编码器：Consistent embedding encoders"></a>2.一致嵌入编码器：Consistent embedding encoders</h4><h5 id="1-几何关系一致性"><a href="#1-几何关系一致性" class="headerlink" title="1. 几何关系一致性"></a>1. 几何关系一致性</h5><blockquote><ol><li>几何关系一致性目的</li></ol></blockquote><p>​ 目的：让<strong>各个视图（view）下得到的Zm 去互相逼近</strong>，越<strong>逼近说明在各个view下得到的Z m 比较一致，进一步说明了网络学到的都是主要信息（</strong>因此才能保证各个view下的Zm都很逼近）</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281517490.png" alt="image-20230328151729436"></p><blockquote><ol start="2"><li>几何关系一致性步骤</li></ol></blockquote><p>​ 步骤：</p><ol><li><strong>降维</strong> ：Zm包含了几乎所有的原始信息，不适合直接进行多视图融合。使用一致的聚类层来学习一个由所有 Zm 自适应集成的公共聚类嵌入 Hm。：<code>Hm被映射到低维空间Zm中</code>。</li><li><strong>计算几何相似度</strong>：假设 <code>Zm 和 Zb 是从一致嵌入编码器获得的视图 m 和 b 的低维空间特征矩阵</code>。然后我们可以使用<code>它们来计算几何关系相似度</code>。比如：曼哈顿距离，欧氏距离，余弦相似度。</li></ol><blockquote><ol start="3"><li>损失函数</li></ol></blockquote><p><strong>几何关系一致性损失函数：</strong></p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281444772.png" alt="image-20230328144403721"></p><h5 id="2-概率分布的一致性"><a href="#2-概率分布的一致性" class="headerlink" title="2. 概率分布的一致性"></a>2. 概率分布的一致性</h5><blockquote><ol><li>概率分布一致性 目的</li></ol></blockquote><p><strong>目的：</strong></p><p><strong><code>各个view下得到的概率分布矩阵</code>都与<code>总的概率分布矩阵去进行逼近</code>，越<code>逼近说明每个view下得到的概率分布矩阵都是比较好的</code>，从而也说明了网络确实具有较好的鲁棒性</strong></p><blockquote><ol start="2"><li>概率分布一致性</li></ol></blockquote><p>我们还考虑了公共表示 Z 和每个视图的潜在表示 Zm 之间概率分布的一致性.</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281532948.png" alt="image-20230328153218886"></p><p>对于 <code>i 样本和 j</code> 样本，我们使用 Student 的 <code>t 分布</code>作为核心来度量<code>嵌入点 hi 和聚类中心向量 μj</code> 之间的相似性，如下所示：</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281549859.png" alt="image-20230328154933804"></p><p><code>qij</code>可以看作是将<code>样本i分配</code>给<code>聚类j的概率</code>，即软分配。</p><p><strong><em>qiu\</em>表示节点i属于簇u的概率，将其看作是每个节点的软聚类分配标签，如果值越大，那么可信度越高</strong> 。通过平方运算将这种可信度放大：</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281550507.png" alt="image-20230328155055468"></p><blockquote><ol start="3"><li>损失函数</li></ol></blockquote><p>MAGCN论文的损失函数没有采用KL散度</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281603339.png" alt="image-20230328160332300"></p><h4 id="3-总损失函数"><a href="#3-总损失函数" class="headerlink" title="3. 总损失函数"></a>3. 总损失函数</h4><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281603819.png" alt="image-20230328160343775"></p><p>我们根据<code>辅助分布P预测每个节点的簇</code>。对于<code>节点i，它的簇可以用pi计算</code>，其中概<code>率值最高的索引是i的簇</code>。因此我们可以得到节点 i 的簇标签为：</p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281606881.png" alt="image-20230328160642832"></p><h3 id="4-实验结果分析"><a href="#4-实验结果分析" class="headerlink" title="4. 实验结果分析"></a>4. 实验结果分析</h3><h4 id="1-MAGCN实验"><a href="#1-MAGCN实验" class="headerlink" title="1. MAGCN实验"></a>1. MAGCN实验</h4><blockquote><ol><li>数据 集</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281615220.png" alt="image-20230328161019647"></p><blockquote><ol start="2"><li>多视图的构建</li></ol></blockquote><p><code>一般的图结构数据库包含一个图和一个属性，目前还没有真正的具有多视图属性的图结构数据。</code></p><p><code>图结构数据的属性为0, 1，它们是离散结构的也是片面描述的</code>。<code>为了更丰富地描述图结构，我们通过改变其操作使属性连续</code>。</p><p><code>受多图的启发</code>，它<code>自己构造另一个图</code>，我们<code>通过原始属性构造额外的属性视图</code>。我们<code>使用快速傅立叶变换 (FFT)、Gabor 变换、欧拉变换和笛卡尔积在视图 1 的基础上构建视图 2</code>:</p><p><strong>视图 2 由笛卡尔积构成</strong></p><blockquote><ol start="3"><li>实验参数</li></ol></blockquote><p>我们为所有三个数据库使用了<code>两层多视图属性图卷积编码器</code>,在多视图<code>图卷积自动编码器</code>中使用非线性<code>激活函数作为 Relu 函数</code>。</p><table><thead><tr><th>数据集</th><th>Cora</th><th>citeseer</th><th>pubmed</th></tr></thead><tbody><tr><td>维度</td><td>512*512</td><td>2000*512</td><td>128 *64</td></tr></tbody></table><blockquote><ol start="3"><li>实验结果</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281607659.png" alt="image-20230328160607888"></p><ol><li><p>MAGCN 在 Cora 和 Citeseer 上比单一视图的 GCN 提高了 5% 以上，在 Pubmed 上提高了 1% 以上，这表明以<code>几何关系和概率分布的一致性</code>来整合<code>不同的视图是有效的</code>。</p></li><li><p><code>单视图图卷积聚类方法</code>：<code>DAEGC 和 GATE，具有相对更好的聚类性能</code>，这表明<code>注意机制根据可训练的注意力权重聚合邻域信息有助于提高聚类性能.</code></p></li><li><p><code>深度多视图聚类方法</code>通过使用<code>图结构信息获得更好的性能</code>。这表明图结构信息可以对聚类做出有益贡献</p></li></ol><h4 id="2-MAGCN-消融实验"><a href="#2-MAGCN-消融实验" class="headerlink" title="2. MAGCN 消融实验"></a>2. MAGCN 消融实验</h4><blockquote><ol><li>概率分布一致性分析</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281659612.png" alt="image-20230328165930571"></p><ol><li><p>在初始迭代中，<code>随机初始化使得每个类别的概率基本相似，无法找出样本属于哪个类别</code>。</p></li><li><p>在第三类中的概率随着迭代次数的增加而增加，<code>Z、Z1、Z2上的概率分布趋于一致</code>，说明理想的<code>多视角描述特征Z</code>是逐渐学习到的.</p></li></ol><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281702981.png" alt="image-20230328170201913"></p><blockquote><ol start="2"><li>参数的影响</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281646531.png" alt="image-20230328164648475"></p><p>对geo 和pro 的超参数分析：</p><p>我们保持reconstruction loss的正则<code>参数1不变</code>，<code>改变</code>模型中<code>几何关系一致性和概率分布一致性</code>的正则<code>参数2和3。</code></p><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281648886.png" alt="image-20230328164843840"></p><blockquote><ol start="3"><li>不同视图view2构造方法分析</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/GhyJn0/2023images/master/202303281643894.png" alt="image-20230328164336848"></p><ol><li>对于所有类型的视图 2，<code>两个视图的聚类结果优于单个视图 1（用黑线标记）的情</code>况。</li><li>对于视图 2（红线标记），<code>笛卡尔积方式</code>比其<code>他构造方式效果更好</code>。</li></ol><h2 id="4-参考链接"><a href="#4-参考链接" class="headerlink" title="4.参考链接"></a>4.参考链接</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_45073095/article/details/123531973">论文学习–Multi-View Attribute Graph Convolution Networks for Clustering(MAGCN)_爱啊岛呀~的博客-CSDN博客</a></p><p>t-student 分布推导过程：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/kailugaji/p/14300272.html">关于“Unsupervised Deep Embedding for Clustering Analysis”的优化问题 - 凯鲁嘎吉 - 博客园 (cnblogs.com)</a></p><p>消融实验：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44065652/article/details/123527844">https://blog.csdn.net/weixin_44065652/article/details/123527844</a></p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">FSRM</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://hexofox.gitee.io/2023/03/26/MAGCN/">https://hexofox.gitee.io/2023/03/26/MAGCN/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">FSRM</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/%E8%AE%BA%E6%96%87/"><span class="chip bg-color">论文</span> </a><a href="/tags/%E6%B7%B1%E5%BA%A6%E8%81%9A%E7%B1%BB/"><span class="chip bg-color">深度聚类</span> </a><a href="/tags/%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">图深度学习</span> </a><a href="/tags/%E5%A4%9A%E8%A7%86%E5%9B%BE/"><span class="chip bg-color">多视图</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/2023/04/02/MVGRL/"><div class="card-image"><img src="/medias/featureimages/24.jpg" class="responsive-img" alt="Contrastive Multi-View Representation Learning on Graphs"> <span class="card-title">Contrastive Multi-View Representation Learning on Graphs</span></div></a><div class="card-content article-content"><div class="summary block-with-text">论文阅读05-Contrastive Multi-View Representation Learning on Graphs：MVRLG论文信息论文地址： Contrastive Multi-View Representation Lea</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2023-04-02 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> FSRM</span></div></div><div class="card-action article-tags"><a href="/tags/%E8%AE%BA%E6%96%87/"><span class="chip bg-color">论文</span> </a><a href="/tags/%E6%B7%B1%E5%BA%A6%E8%81%9A%E7%B1%BB/"><span class="chip bg-color">深度聚类</span> </a><a href="/tags/%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">图深度学习</span> </a><a href="/tags/%E5%A4%9A%E8%A7%86%E5%9B%BE/"><span class="chip bg-color">多视图</span> </a><a href="/tags/%E5%AF%B9%E6%AF%94/"><span class="chip bg-color">对比</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/2023/03/23/R-GAE/"><div class="card-image"><img src="/medias/featureimages/24.jpg" class="responsive-img" alt="R-GAE"> <span class="card-title">R-GAE</span></div></a><div class="card-content article-content"><div class="summary block-with-text">论文信息论文地址：[2107.08562] Rethinking Graph Auto-Encoder Models for Attributed Graph Clustering (arxiv.org) 论文代码：nairouz/R-GA</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2023-03-23 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> FSRM</span></div></div></div></div></div></article></div><script>$("#articleContent").on("copy",function(e){var n,t,o,i;void 0===window.getSelection||(""+(n=window.getSelection())).length<Number.parseInt("120")||(t=document.getElementsByTagName("body")[0],(o=document.createElement("div")).style.position="absolute",o.style.left="-99999px",t.appendChild(o),o.appendChild(n.getRangeAt(0).cloneContents()),"PRE"===n.getRangeAt(0).commonAncestorContainer.nodeName&&(o.innerHTML="<pre>"+o.innerHTML+"</pre>"),i=document.location.href,o.innerHTML+='<br />来源: FSRM<br />文章作者: FSRM<br />文章链接: <a href="'+i+'">'+i+"</a><br />本文章著作权归作者所有，任何形式的转载都请注明出处。",n.selectAllChildren(o),window.setTimeout(function(){t.removeChild(o)},200))})</script><script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{var t,n="prenext-posts";let e=$("#"+"artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#"+n).width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">FSRM</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">67k</span>&nbsp;字<br><br></div><div class="col s12 m4 l4 social-link"><a href="https://github.com/GhyJn0" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50"><i class="fas fa-rss"></i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/libs/materialize/materialize.min.js"></script><script src="/libs/masonry/masonry.pkgd.min.js"></script><script src="/libs/aos/aos.js"></script><script src="/libs/scrollprogress/scrollProgress.min.js"></script><script src="/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0],e=(t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",document.getElementsByTagName("script")[0]);e.parentNode.insertBefore(t,e)}()</script><script src="/libs/others/clicklove.js" async></script><script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async></script><script src="/libs/instantpage/instantpage.js" type="module"></script></body></html>