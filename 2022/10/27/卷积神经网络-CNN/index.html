<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="卷积神经网络:CNN, java,安全,python"><meta name="description" content="一个小白成长史"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>卷积神经网络:CNN | FSRM</title><link rel="icon" type="image/png" href="/favicon.png"><link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="/css/matery.css"><link rel="stylesheet" type="text/css" href="/css/my.css"><script src="/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="FSRM" type="application/atom+xml"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">FSRM</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>归档</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/friends" class="waves-effect waves-light"><i class="fas fa-address-book" style="zoom:.6"></i> <span>友情链接</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">FSRM</div><div class="logo-desc">一个小白成长史</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="/friends" class="waves-effect waves-light"><i class="fa-fw fas fa-address-book"></i> 友情链接</a></li><li><div class="divider"></div></li><li><a href="https://github.com/ghyjn0" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i>Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/ghyjn0" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url(/medias/featureimages/23.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">卷积神经网络:CNN</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">深度学习</span></a></div></div><div class="col s5 right-align"></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2022-10-27</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-12-01</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.9k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 11 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="CNN-卷积神经网络"><a href="#CNN-卷积神经网络" class="headerlink" title="CNN(卷积神经网络)"></a>CNN(卷积神经网络)</h1><h2 id="1-CNN-先验知识"><a href="#1-CNN-先验知识" class="headerlink" title="1.CNN 先验知识"></a>1.CNN 先验知识</h2><h3 id="1-浅层特征和深层特征"><a href="#1-浅层特征和深层特征" class="headerlink" title="1.浅层特征和深层特征"></a>1.浅层特征和深层特征</h3><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155227.jpg" alt="浅层和深层"></p><p><strong>浅层特征优缺点：</strong></p><p><strong>分辨率更高==》更多位置、细节信息，噪声更多。</strong></p><p><strong>深层特征优缺点：</strong></p><p><strong>分辨率低==》细节较差，噪声少，获取整体性信息</strong></p><p>参考链接：</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_62311817/article/details/126064158">(19条消息) 卷积神经网络之“浅层特征”与“深层特征”_一个菜鸟的成长史的博客-CSDN博客_深层特征和浅层特征</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ybdesire/article/details/78837688">(19条消息) 深层神经网络与浅层神经网络的区别_ybdesire的博客-CSDN博客_浅层网络与深层网络的差异</a></p><h3 id="2-CNN发展和人是如何识别图片"><a href="#2-CNN发展和人是如何识别图片" class="headerlink" title="2.CNN发展和人是如何识别图片"></a>2.CNN发展和人是如何识别图片</h3><p>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41792162/article/details/118250917?ops_request_misc=&request_id=&biz_id=102&utm_term=cnn&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~sobaiduweb~default-0-118250917.nonecase&spm=1018.2226.3001.4450">(19条消息) CNN简单介绍及基础知识_Python大视觉的博客-CSDN博客_cnn介绍</a></p><h2 id="2-CNN-为什么出现"><a href="#2-CNN-为什么出现" class="headerlink" title="2.CNN 为什么出现"></a>2.CNN 为什么出现</h2><h3 id="1-全连接网络-缺陷"><a href="#1-全连接网络-缺陷" class="headerlink" title="1.全连接网络 缺陷"></a>1.全连接网络 缺陷</h3><blockquote><p>1.参数量大</p></blockquote><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027160434.jpg" alt="参数量大"></p><p><strong>从图中可以得出结论全连接网络的缺点：</strong></p><ol><li><strong>参数量大</strong>：<strong>计算量大</strong></li><li><strong>过拟合：参数量大 数据就要增加，否则出现过拟合现象</strong></li></ol><blockquote><p>2.不具备特征不变性</p></blockquote><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155245.png" alt="缺点"></p><p><strong>不具备特征不变性：非常依赖于特征的位置：特征位置平移，形变，扭曲敏感。</strong></p><p><strong>全连接网络优点：</strong></p><p><strong><code>输入数据</code>的 <code>所有信息</code>都会得到有效的利用，因为 通过<code>全连接</code>的方式，<code>每一点信息都会对训练过程</code>做出“ 贡献 ”</strong></p><p><strong>参考链接：</strong></p><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kensporger/p/12267319.html#5hDZaDcy">冬日曙光——回溯CNN的诞生 - KenSporger - 博客园 (cnblogs.com)</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39670246/article/details/110604745">(19条消息) cnn神经网络_为什么会提出卷积神经网络(CNN)?CNN的基本框架_weixin_39670246的博客-CSDN博客</a></p><h3 id="2-卷积神经网络特征"><a href="#2-卷积神经网络特征" class="headerlink" title="2.卷积神经网络特征"></a>2.卷积神经网络特征</h3><blockquote><p>1.参数共享</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20200722163245482.gif" alt="参数共享"></p><p><strong>参数共享</strong>是指在<strong>同一个模型的不同模块</strong>中<strong>使用相同的参数</strong>，它是<strong>卷积运算的 固有属性</strong>。<strong>全连接网络中</strong>，计算<strong>每层的输出</strong>时，<strong>权值参数矩阵中的每个元素只</strong>作用于某个<strong>输入元素一次</strong>；而在<strong>卷积神经网络</strong>中，<strong>卷积核中的每一个元素将作用于 每一次局部输入的特定位置上</strong>。根据<strong>参数共享的思想</strong>，我们<strong>只需要学习一组参数集合</strong>，而<strong>不需要针对每个位置的每个参数都进行优化</strong>，从而大大降低了模型的存 储需求。</p><p><strong>参数共享的物理意义</strong>是<strong>使得卷积层具有平移等变性</strong>。假如图像中有一只猫， 那么无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经 网络的输出对于平移变换来说应当是等变的。</p><p>参考博客：</p><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/MinPage/p/14237303.html">CNN中的卷积操作与参数共享 - 箐茗 - 博客园 (cnblogs.com)</a></p><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/450030946">卷积神经网络中的稀疏交互和参数共享 - 知乎 (zhihu.com)</a></p><blockquote><p>2.局部连接</p></blockquote><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027160420.jpg" alt="局部链接"></p><p><strong>局部感知</strong>：<strong>只连接相邻层的部分神经元</strong></p><p><strong>图像局部范围内的像素有较高的关联性</strong>，随着<strong>距离的加长</strong>，<strong>像素的信息相关性会降低</strong>，因此在<strong>连接神</strong></p><p><strong>经元时</strong>，<strong>不需要</strong>同时<strong>输入全局信息给单个神经元</strong>，只需要<strong>感知图像的局部信息</strong>，随着<strong>层数的增</strong></p><p><strong>加深层</strong>的<strong>神经元会拥有广阔的感受野</strong>，使得<strong>模型能准确表征图像的全局信息</strong>。</p><p><code>局部感受野，相比于全连接，它同时考虑某个局部区域，这样更容易提取到想要的特征</code></p><blockquote><p>3.特征不变性</p></blockquote><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155312.jpg" alt="不变性"></p><p>这里面尺寸大小不是放大或者缩小的图片。把一个图片放大或者缩小卷积网络是无法识别出来的</p><p>参考博客：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/kensporger/p/12267319.html#NZ574FW6">冬日曙光——回溯CNN的诞生 - KenSporger - 博客园 (cnblogs.com)</a></p><p>参考视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1sf4y1o7tg/?spm_id_from=333.880.my_history.page.click">卷积神经网络中的卷积、池化、局部连接以及权重共享-跟李沐老师动手学深度学习_哔哩哔哩_bilibili</a></p><h2 id="3-CNN-结构"><a href="#3-CNN-结构" class="headerlink" title="3.CNN 结构"></a>3.CNN 结构</h2><h3 id="1-输入层（数据预处理）"><a href="#1-输入层（数据预处理）" class="headerlink" title="1.输入层（数据预处理）"></a>1.输入层（数据预处理）</h3><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155736.jpg" alt="input"></p><p>对于<strong>CNN</strong>来说，<strong>一般只会做去均值+归一化</strong>，尤其是我们<strong>输入的是图像数据</strong>，我们<strong>往往只做去均值</strong>，这是<strong>为了保持原图像数据的完整性，避免失真</strong>。</p><blockquote><p>数据预处理优点</p></blockquote><pre><code>1. 简单的减均值操作都是可以加速收敛的
1. 减少计算量，节省资源
1. 降低陷入过拟合的问题
</code></pre><p>参考博客:</p><p><strong>重点推荐</strong>：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/MinPage/p/13992071.html">深度学习之数据预处理 - 箐茗 - 博客园 (cnblogs.com)</a></p><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kongweisi/p/10987870.html">CNN（卷积神经网络）入门 - 胖白白 - 博客园 (cnblogs.com)</a></p><h3 id="2-卷积层（提取特征）"><a href="#2-卷积层（提取特征）" class="headerlink" title="2.卷积层（提取特征）"></a>2.卷积层（<strong>提取特征</strong>）</h3><blockquote><p>卷积层定义</p></blockquote><p><strong>卷积层（Convolutional layer）</strong>，<strong>卷积神经网络中每层卷积层由若干卷积单元(卷积核)组成</strong>，<strong>每个卷积单元的参数</strong>都是通过<strong>反向传播算法优化得到的</strong>。</p><blockquote><p>卷积运算目的</p></blockquote><p><strong>卷积运算的目</strong>的是<strong>提取输入的不同特征</strong>，<strong>第一层卷积层</strong>可能只能<strong>提取一些低级的特征如边缘</strong>、<strong>线条和角等层级</strong>，<strong>更多层的网路</strong>能从低级特征中迭代<strong>提取更复杂的特征</strong></p><blockquote><p>如何卷积运算</p></blockquote><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155607.jpg" alt="计算卷积"></p><p><strong><code>卷积核上的每个元素和对应输入数据的元素相乘，最后把所有的结果相加即卷积的结果</code></strong></p><p><img src="https://img-blog.csdnimg.cn/20200722163245482.gif" alt="卷积运算例子"></p><h3 id="3-池化层（压缩数据和参数，减小过拟合，降低网络的复杂度）"><a href="#3-池化层（压缩数据和参数，减小过拟合，降低网络的复杂度）" class="headerlink" title="3.池化层（压缩数据和参数，减小过拟合，降低网络的复杂度）"></a>3.池化层（<strong>压缩数据和参数，减小过拟合，降低网络的复杂度</strong>）</h3><blockquote><p>池化层 作用</p></blockquote><p><strong>池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</strong></p><p>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_50645064/article/details/126181193">(19条消息) YOLOv5基础知识点——卷积神经网络_MUTA️的博客-CSDN博客_卷积神经网络yolov5</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40519315/article/details/105115657?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-105115657-blog-107516873.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-105115657-blog-107516873.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=1">(19条消息) 卷积神经网络CNN(卷积池化、感受野、共享权重和偏置、特征图)_玖零猴的博客-CSDN博客_卷积神经网络偏置</a></p><h4 id="1-最大池化"><a href="#1-最大池化" class="headerlink" title="1.最大池化"></a>1.最大池化</h4><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155545.jpg" alt="max"></p><h4 id="2-平均池化"><a href="#2-平均池化" class="headerlink" title="2.平均池化"></a>2.平均池化</h4><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155552.jpg" alt="avg"></p><p><strong>注意事项：</strong></p><p><strong><code>**池化层改变图像大小，不改变通道数：即输入是几通道，输出就是几通道**</code></strong></p><h3 id="4-激活层（增加非线性分割能力）"><a href="#4-激活层（增加非线性分割能力）" class="headerlink" title="4.激活层（增加非线性分割能力）"></a>4.激活层（<strong>增加非线性分割能力</strong>）</h3><blockquote><p>作用</p></blockquote><p><strong>作用：把卷积层输出结果做非线性映射</strong></p><blockquote><p>Relu激活函数</p></blockquote><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155534.jpg" alt="relu"></p><p><strong>优点：</strong></p><p><strong>解决梯度消失问题</strong></p><p><strong>计算速度快，只需要判断输入是否大与0.（SGD 的求解速度快于sigmoid 和tanh)</strong></p><p><strong>卷积神经网络激活函数选择忠告：</strong></p><p><strong>CNN 慎用sigmoid,优先使用RELU。</strong></p><p>激活函数作用可视化：<a target="_blank" rel="noopener" href="http://playground.tensorflow.org/">http://playground.tensorflow.org/</a></p><p>参考博客：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/kongweisi/p/10987870.html">CNN（卷积神经网络）入门 - 胖白白 - 博客园 (cnblogs.com)</a></p><h3 id="5-全连接层"><a href="#5-全连接层" class="headerlink" title="5.全连接层"></a>5.全连接层</h3><blockquote><p>作用</p></blockquote><p><strong>全连接层在整个卷积神经网络中起到“分类器”的作用</strong></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pA4y1S7Z1/?spm_id_from=333.788&vd_source=5e8f069711510b3788382a0a03ff38e5">卷积神经网络原理 - 02 - 深层表示学习_哔哩哔哩_bilibili</a></p><p>重点推荐 李沐：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1L64y1m7Nh/?spm_id_from=333.999.0.0">19 卷积层【动手学深度学习v2】_哔哩哔哩_bilibili</a></p><h2 id="4-CNN-参数"><a href="#4-CNN-参数" class="headerlink" title="4.CNN 参数"></a>4.CNN 参数</h2><h3 id="1-卷积核"><a href="#1-卷积核" class="headerlink" title="1.卷积核"></a>1.卷积核</h3><h4 id="1-大小：卷积核大小为奇数"><a href="#1-大小：卷积核大小为奇数" class="headerlink" title="1.大小：卷积核大小为奇数"></a>1.大小：卷积核大小为奇数</h4><p>*<em>1</em>1**</p><p>*<em>3</em>3**</p><p>*<em>5</em>5**</p><h4 id="2-单通道"><a href="#2-单通道" class="headerlink" title="2.单通道"></a>2.单通道</h4><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155623.jpg" alt="单通道"></p><h4 id="3-多通道"><a href="#3-多通道" class="headerlink" title="3.多通道"></a>3.多通道</h4><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155629.jpg" alt="多通道"></p><p><code>注意事项：</code></p><p><strong>卷积核通道要和输入特征图的通道数相同</strong></p><p><strong>有多少个卷积核就有多少个输出通道</strong></p><h3 id="4-步长和填充"><a href="#4-步长和填充" class="headerlink" title="4.步长和填充"></a>4.步长和填充</h3><p><strong>步长：卷积核每次移动大小</strong></p><p>**填充：输入和输出具有相同的大小 **</p><p><strong>参考链接：CNN 可视化调参</strong></p><p><a target="_blank" rel="noopener" href="https://poloclub.github.io/cnn-explainer/#article-flatten">CNN Explainer (poloclub.github.io)</a></p><h3 id="5-卷积核大小-步长-填充-计算输出尺寸"><a href="#5-卷积核大小-步长-填充-计算输出尺寸" class="headerlink" title="5.卷积核大小 步长 填充 计算输出尺寸"></a>5.卷积核大小 步长 填充 计算输出尺寸</h3><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155706.jpg" alt="compute"></p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155641.jpg" alt="步长填充计算输出"></p><pre class="language-python"><code class="language-python">oh<span class="token punctuation">(</span>高<span class="token punctuation">)</span><span class="token operator">=</span>（<span class="token number">5</span><span class="token operator">+</span><span class="token number">2</span><span class="token operator">*</span><span class="token number">1</span><span class="token operator">-</span><span class="token number">3</span>）<span class="token operator">/</span><span class="token number">1</span><span class="token operator">+</span><span class="token number">1</span><span class="token operator">=</span><span class="token number">5</span>
ow<span class="token punctuation">(</span>宽<span class="token punctuation">)</span><span class="token operator">=</span>（<span class="token number">5</span><span class="token operator">+</span><span class="token number">2</span><span class="token operator">*</span><span class="token number">1</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1</span><span class="token operator">+</span><span class="token number">1</span><span class="token operator">=</span><span class="token number">5</span>
</code></pre><h2 id="5-CNN实践：LeNet-5手写数字识别"><a href="#5-CNN实践：LeNet-5手写数字识别" class="headerlink" title="5.CNN实践：LeNet-5手写数字识别"></a>5.CNN实践：LeNet-5手写数字识别</h2><h3 id="1-LeNet-5结构"><a href="#1-LeNet-5结构" class="headerlink" title="1.LeNet-5结构"></a>1.LeNet-5结构</h3><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155656.jpg" alt="lenet-5"></p><h3 id="2-LeNet-5手写数字识别"><a href="#2-LeNet-5手写数字识别" class="headerlink" title="2.LeNet-5手写数字识别"></a>2.LeNet-5手写数字识别</h3><blockquote><p>1.导入相关库</p></blockquote><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> mnist
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
</code></pre><blockquote><p>2.搭建LeNet-5</p></blockquote><pre class="language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 构造LeNet-5卷积神经网络</span>
<span class="token keyword">class</span> <span class="token class-name">LeNet5</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>in_channel<span class="token punctuation">,</span>output<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># in_channels输入通道数，output 输出</span>
        super<span class="token punctuation">(</span>LeNet5<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true">#inchannels 输入通道，outchannels输出通道，kernel_size卷积核大小，stride 步长，padding填充尺寸</span>
        self<span class="token punctuation">.</span>layer1<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channel<span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 6 28*28</span>
            nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#激活函数</span>
            nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 6 14*14</span>
        <span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>layer2<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#16  10*10</span>
            nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#激活函数</span>
            nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 16 5*5</span>
        <span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>layer3<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true">#120 1*1</span>
        <span class="token punctuation">)</span>
        
        <span class="token comment" spellcheck="true">#nn.Linear()：用于设置网络中的全连接层，需要注意的是全连接层的输入与输出都是二维张量</span>
        self<span class="token punctuation">.</span>layer4<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">,</span>out_features<span class="token operator">=</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">84</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span>output<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true"># 前向传播</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>input<span class="token operator">=</span>x<span class="token punctuation">,</span> start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 把120个输出拼接出来</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
      
</code></pre><blockquote><p>3.数据下载和预处理</p></blockquote><pre class="language-python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">#cpu</span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
train_set <span class="token operator">=</span> mnist<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">"./mnist_data"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>
test_set <span class="token operator">=</span> mnist<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">"./mnist_data"</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>train_set<span class="token punctuation">)</span><span class="token punctuation">,</span>len<span class="token punctuation">(</span>test_set<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">#结果 60000 10000</span>

train_batch_size <span class="token operator">=</span> <span class="token number">300</span>
test_batch_size <span class="token operator">=</span> <span class="token number">50</span>

train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>train_set<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>train_batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
test_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>test_set<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>test_batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre><blockquote><ol start="4"><li>查看部分数据</li></ol></blockquote><pre class="language-python"><code class="language-python">examples <span class="token operator">=</span> enumerate<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span>
batch_idex<span class="token punctuation">,</span> <span class="token punctuation">(</span>example_data<span class="token punctuation">,</span> example_label<span class="token punctuation">)</span> <span class="token operator">=</span> next<span class="token punctuation">(</span>examples<span class="token punctuation">)</span>
sample_set <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>example_data<span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>sample_set<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Truth: &amp;#123;&amp;#125;"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>example_label<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><p><img src="../../../../../11345/AppData/Roaming/Typora/typora-user-images/image-20221027135830388.png" alt="image-20221027135830388"></p><blockquote><p>5.模型参数</p></blockquote><pre class="language-python"><code class="language-python">model <span class="token operator">=</span> LeNet5<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

lr <span class="token operator">=</span> <span class="token number">0.3</span><span class="token comment" spellcheck="true">#学习率</span>
num_epoches <span class="token operator">=</span> <span class="token number">100</span><span class="token comment" spellcheck="true"># 迭代次数</span>
momentum <span class="token operator">=</span> <span class="token number">0.8</span>

criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 交叉熵</span>

optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#优化器选择sgd</span>
scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 自动调节学习率</span>
</code></pre><blockquote><p>6.开始训练模型</p></blockquote><pre class="language-python"><code class="language-python">eval_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
eval_acces <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_epoches<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token comment" spellcheck="true">#     if epoch % 5 == 0:</span>
<span class="token comment" spellcheck="true">#         optimizer.param_groups[0]['lr'] *= 0.1</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"学习率lr："</span><span class="token punctuation">,</span>optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span>
   <span class="token comment" spellcheck="true"># print("优化器中参数：",optimizer.param_groups)</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> imgs<span class="token punctuation">,</span> labels <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>
       <span class="token comment" spellcheck="true"># print(imgs,labels)</span>
        imgs<span class="token punctuation">,</span> labels <span class="token operator">=</span> imgs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        predict <span class="token operator">=</span> model<span class="token punctuation">(</span>imgs<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predict<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># back propagation</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 疑问 1为啥每次都要把梯度清零</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    eval_loss <span class="token operator">=</span> <span class="token number">0</span>
    eval_acc <span class="token operator">=</span> <span class="token number">0</span>
    model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> imgs<span class="token punctuation">,</span> labels <span class="token keyword">in</span> test_loader<span class="token punctuation">:</span>
        imgs<span class="token punctuation">,</span> labels <span class="token operator">=</span> imgs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        predict <span class="token operator">=</span> model<span class="token punctuation">(</span>imgs<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predict<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># record loss</span>
        eval_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># record accurate rate</span>
        result <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>predict<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        acc_num <span class="token operator">=</span> <span class="token punctuation">(</span>result <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        acc_rate <span class="token operator">=</span> acc_num <span class="token operator">/</span> imgs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        eval_acc <span class="token operator">+=</span> acc_rate

    eval_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>eval_loss <span class="token operator">/</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">)</span>
    eval_acces<span class="token punctuation">.</span>append<span class="token punctuation">(</span>eval_acc <span class="token operator">/</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch: &amp;#123;&amp;#125;'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'loss： &amp;#123;&amp;#125;'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>eval_loss <span class="token operator">/</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'accurate rate: &amp;#123;&amp;#125;'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>eval_acc <span class="token operator">/</span> len<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>


</code></pre><blockquote><p>7.tanh激活下的损失和准确图像</p></blockquote><pre class="language-python"><code class="language-python">plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'evaluation loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>len<span class="token punctuation">(</span>eval_losses<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eval_losses<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155801.png" alt="loss"></p><pre class="language-python"><code class="language-python">plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'evaluation acc'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>len<span class="token punctuation">(</span>eval_acces<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eval_acces<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20221027155808.png" alt="acc"></p><h2 id="6-CNN-发展"><a href="#6-CNN-发展" class="headerlink" title="6.CNN 发展"></a>6.CNN 发展</h2><h3 id="1-ALeNet"><a href="#1-ALeNet" class="headerlink" title="1.ALeNet"></a>1.ALeNet</h3><h3 id="2-VGG"><a href="#2-VGG" class="headerlink" title="2.VGG"></a>2.VGG</h3><h3 id="3-GooGLenet"><a href="#3-GooGLenet" class="headerlink" title="3.GooGLenet"></a>3.GooGLenet</h3><h3 id="4-ResNet"><a href="#4-ResNet" class="headerlink" title="4.ResNet"></a>4.ResNet</h3><p>参考链接：</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44944722/article/details/126713365">(19条消息) 经典卷积神经网络模型 - ResNet_WBZhang2022的博客-CSDN博客_resnet卷积</a></p><h2 id="7-参考链接"><a href="#7-参考链接" class="headerlink" title="7.参考链接"></a>7.参考链接</h2><h3 id="1-视频参考"><a href="#1-视频参考" class="headerlink" title="1.视频参考"></a>1.视频参考</h3><blockquote><p>动画讲解</p></blockquote><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1R5411w715/?spm_id_from=333.788&vd_source=5e8f069711510b3788382a0a03ff38e5">【数之道 08】走进”卷积神经网络”，了解图像识别背后的原理_哔哩哔哩_bilibili</a></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1fY411H7g8/?spm_id_from=333.788&vd_source=5e8f069711510b3788382a0a03ff38e5">【卷积神经网络】8分钟搞懂CNN，动画讲解喜闻乐见_哔哩哔哩_bilibili</a></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1oL411t72z/?spm_id_from=333.788.recommend_more_video.1&vd_source=5e8f069711510b3788382a0a03ff38e5">从图像识别走进卷积神经网络——可能是全网最通俗易懂的深度学习课程_哔哩哔哩_bilibili</a></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1VV411478E/?spm_id_from=333.788.recommend_more_video.1">从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变_哔哩哔哩_bilibili</a></p><blockquote><p>CNN 模型可视化网站</p></blockquote><p><a target="_blank" rel="noopener" href="https://poloclub.github.io/cnn-explainer/#article-pooling">CNN Explainer (poloclub.github.io)</a></p><p>激活函数 可视化调参</p><p><a target="_blank" rel="noopener" href="http://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=10&networkShape=4,5,3,2&seed=0.87043&showTestData=false&discretize=false&percTrainData=90&x=true&y=true&xTimesY=true&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false">A Neural Network Playground (tensorflow.org)</a></p><blockquote><p>理论视频讲解</p></blockquote><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1MS4y1b7DU/?spm_id_from=333.788&vd_source=5e8f069711510b3788382a0a03ff38e5">卷积神经网络原理 - 01 - 表示学习_哔哩哔哩_bilibili</a></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1L64y1m7Nh/?spm_id_from=333.999.0.0">19 卷积层【动手学深度学习v2】_哔哩哔哩_bilibili</a></p><h3 id="2-博客参考链接"><a href="#2-博客参考链接" class="headerlink" title="2.博客参考链接"></a>2.博客参考链接</h3></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">FSRM</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://hexofox.gitee.io/2022/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN/">https://hexofox.gitee.io/2022/10/27/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">FSRM</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">深度学习</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/2022/11/07/%E5%85%AD%E7%BA%A7%E7%BF%BB%E8%AF%91/"><div class="card-image"><img src="/medias/featureimages/14.jpg" class="responsive-img" alt="六级翻译"> <span class="card-title">六级翻译</span></div></a><div class="card-content article-content"><div class="summary block-with-text">六级翻译技巧方法1.找主干-添枝加叶-检查回读 谁是什么 谁做了什么 什么被做(be done) 修饰成分有动词 用定语从句 2.无主语 添加 主语：people ,they ,we, he ,his,it 换成被动 there be</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-11-07 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> FSRM</span></div></div><div class="card-action article-tags"><a href="/tags/%E5%85%AD%E7%BA%A7/"><span class="chip bg-color">六级</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/2022/10/22/%E8%B7%91%E6%AD%A5/"><div class="card-image"><img src="/medias/featureimages/4.jpg" class="responsive-img" alt="跑步"> <span class="card-title">跑步</span></div></a><div class="card-content article-content"><div class="summary block-with-text">跑步相关知识1.跑步姿势1.前脚掌2. 后脚掌3.错误 “刹车效应”2.鞋子挑选3.不正确的跑步1.心率2.膝盖</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-10-22 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> FSRM</span></div></div><div class="card-action article-tags"><a href="/tags/%E7%94%9F%E6%B4%BB/"><span class="chip bg-color">生活</span></a></div></div></div></div></article></div><script>$("#articleContent").on("copy",function(e){var n,t,o,i;void 0===window.getSelection||(""+(n=window.getSelection())).length<Number.parseInt("120")||(t=document.getElementsByTagName("body")[0],(o=document.createElement("div")).style.position="absolute",o.style.left="-99999px",t.appendChild(o),o.appendChild(n.getRangeAt(0).cloneContents()),"PRE"===n.getRangeAt(0).commonAncestorContainer.nodeName&&(o.innerHTML="<pre>"+o.innerHTML+"</pre>"),i=document.location.href,o.innerHTML+='<br />来源: FSRM<br />文章作者: FSRM<br />文章链接: <a href="'+i+'">'+i+"</a><br />本文章著作权归作者所有，任何形式的转载都请注明出处。",n.selectAllChildren(o),window.setTimeout(function(){t.removeChild(o)},200))})</script><script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{var t,n="prenext-posts";let e=$("#"+"artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#"+n).width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">FSRM</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">65.2k</span>&nbsp;字<br><br></div><div class="col s12 m4 l4 social-link"><a href="https://github.com/GhyJn0" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50"><i class="fas fa-rss"></i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/libs/materialize/materialize.min.js"></script><script src="/libs/masonry/masonry.pkgd.min.js"></script><script src="/libs/aos/aos.js"></script><script src="/libs/scrollprogress/scrollProgress.min.js"></script><script src="/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0],e=(t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",document.getElementsByTagName("script")[0]);e.parentNode.insertBefore(t,e)}()</script><script src="/libs/others/clicklove.js" async></script><script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async></script><script src="/libs/instantpage/instantpage.js" type="module"></script></body></html>