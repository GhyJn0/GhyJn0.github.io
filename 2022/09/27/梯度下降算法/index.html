<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="机器学习:梯度下降算法, java,安全,python"><meta name="description" content="一个小白成长史"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>机器学习:梯度下降算法 | FSRM</title><link rel="icon" type="image/png" href="/favicon.png"><link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="/css/matery.css"><link rel="stylesheet" type="text/css" href="/css/my.css"><script src="/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="FSRM" type="application/atom+xml"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">FSRM</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>归档</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/friends" class="waves-effect waves-light"><i class="fas fa-address-book" style="zoom:.6"></i> <span>友情链接</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">FSRM</div><div class="logo-desc">一个小白成长史</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="/friends" class="waves-effect waves-light"><i class="fa-fw fas fa-address-book"></i> 友情链接</a></li><li><div class="divider"></div></li><li><a href="https://github.com/ghyjn0" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i>Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/ghyjn0" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url(/medias/featureimages/7.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">机器学习:梯度下降算法</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">机器学习</span></a></div></div><div class="col s5 right-align"></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2022-09-27</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-09-29</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 2.5k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 8 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220927211653.png" alt="image-20220927211653320"></p><h2 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h2><p>　　在经典的随机梯度下降算法（批量梯度下降）中，迭代下降公式是</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929075945.png" alt="image-20220929075937929"></p><p>　　以</p><p>一元线性回归的目标函数为例</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080202.png" alt="image-20220929080202871"></p><p>　　其梯度表达为</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080217.png" alt="image-20220929080217757"></p><p>　　可以看到，==这里的梯度计算，使用了所有的样本数据。倘若数据集有 1000 组数据，那就需要计算 1000 次才可以得到梯度，倘若数据集有一亿组数据，就需要计算一亿次，其时间复杂度是 O(n)== 。当样本数据较多时，对于模型的求解，学习一次的过程是很浪费时间的。　</p><hr><p>　　举例：使用只含有==一个特征的线性回归==来展开。<br>　　线性回归的假设函数为：</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080227.png" alt="image-20220929080227333"></p><p>　　其中 i=1,2,…,n，其中 n 表示样本数。<br>　　对应的目标函数（代价函数）即为：</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080242.png" alt="image-20220929080242727"></p><p>　　==批量梯度下降法是指在每一次迭代时使用所有样本来进行梯度的更新==。</p><p>　　步骤如下：<br>　　(1)对目标函数求偏导</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080253.png" alt="image-20220929080253628"></p><p>　　其中 i=1,2,…,n。n 表示样本数，j=0,1 表示特征数，这里使用了偏置项 x(i)0=1<br>　　(2)每次迭代对参数进行更新：　</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080304.png" alt="image-20220929080303958"></p><p>　　注意：这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。<br>　　==优点：==<br>  (1)一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。<br>  (2)由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。<br>  ==缺点==：<br>  (1)当样本数目 nn 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。<br>  从迭代的次数上来看，BGD迭代的次数相对较少。</p><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>　　为解决批量梯度下降法学习一次浪费时间的问题，因此，可以==在所有的样本数据中选择随机的一个实例==，用这个实例所包含的数据计算“梯度”。此时的梯度为</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080328.png" alt="image-20220929080328846"></p><p>　　其中 (xi,yi) 是一个随机选中的样本。<br>　　到了这里，可能会存在一定的疑问，因为目标函数（代价函数）</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080337.png" alt="image-20220929080336986"></p><p>　　其梯度并不是</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080343.png" alt="image-20220929080343126"></p><p>　　==那么这个方向还是不是可以使目标函数值下降的方向？只能说，对于一次迭代而言，不一定，但是站在宏观的角度去考虑，最后还是很有机会收敛到近似最优解的。==<br>　　事实上，目标函数可以写成</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080348.png" alt="image-20220929080348748"></p><p>　　所以梯度则是</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080354.png" alt="image-20220929080354688"></p><p>　　==这时，优化目标是所有样本的损失函数之和，所以在梯度下降时，自然而然是朝着使总的偏差缩小的方向去移动的。而对于随机梯度下降，每一步迭代的优化目标函数不是始终不变的，其变化的范围就是==</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080402.png" alt="image-20220929080402205"></p><p>　　在第 i步，随机地选中 S(i)作为优化目标，其梯度便是</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080408.png" alt="image-20220929080408409"></p><p>　　而在第 i+1步，我们的优化目标可能就变成了</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080422.png" alt="image-20220929080422214"></p><p>　　此时，梯度也自然变成了</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080428.png" alt="image-20220929080428619"></p><p>　　====显然，随机梯度下降迭代过程中，考虑的下降方向并不是全局下降方向，而是使得某个随机选中的样本的损失函数下降的方向。在一步迭代中，这种局部样本的下降未必会导致全局损失的下降，但是当迭代次数足够的时候，绝大部分样本都会被考虑到，最终一步一步走向全局最优解。==<br>　　==所以，随机梯度下降相对于梯度下降而言，其根本区别在于每一步迭代时需要优化的目标函数不同。对于经典的梯度下降，其每一步的目标函数（损失函数）是一样的，即所有样本的（平均）损失函数之和。而对于随机梯度下降而言，其每一步的目标函数是被随机选中的某个样本的损失函数，并不是一直不变的。====<br>　　可以通过下面这个视频直观地感受一下随机梯度下降。</p><p>　　<a target="_blank" rel="noopener" href="https://www.zhihu.com/zvideo/1308443683624992768">SGD可视化视频</a></p><p>　　上面的每个小球，可以将其理解为随机梯度下降过程中由于随机性而带来的迭代情况的分支。正是由于这种随机性的存在，每个球可以较为自由地选择运动方向，有些就停在某个位置，有些则一路向下。当迭代的次数足够多时，总会有某个球的路径十分顺畅，最终到达全局最优解的附近。随机梯度下降相对于经典梯度下降，其逃离局部最优的能力更强。因为一旦到达了某个样本的局部最优，随着目标函数的更换，很可能不再是另一个样本的局部最优，迭代就可以继续进行。<br>　　当然，==随机梯度下降的缺点也是存在的，即它很可能无法收敛到全局最优解。什么是全局最优，==是</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080441.png" alt="image-20220929080441567"></p><p>达到最小嘛？还是每一个 S(i) 都无法继续下降？一般而言，前者可能更容易衡量一些，我们也更偏向于使用总体的最优作为全局最优，而非每一个样本的最优。而对于随机梯度下降，即使已经达到了总体全局最优，对于某些样本而言，其可能依然可以继续下降，所以一旦选中了这些样本，就要偏离全局最优点。所以随机梯度下降最终的收敛性确实值得考虑。<br>　　但总的来说，==随机梯度下降还是很不错的，特别是对于大样本的处理情况，每一次迭代中 O(1) 的计算开销无疑会轻松很多，至于最终的收敛问题，则要根据迭代次数，终止准则等进行一个衡量取舍啦。==</p><hr><p>　　<strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是<strong>每次迭代</strong>使用<strong>一个样本</strong>来对参数进行更新。使得训练速度加快。</p><p>  对于<strong>一个样本</strong>的目标函数为：</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080448.png" alt="image-20220929080448441"></p><p>　　(1)对目标函数求偏导：</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080455.png" alt="image-20220929080455602"></p><p>　　(2)参数更新：</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080503.png" alt="image-20220929080503544"></p><p>　　注意：这里不再有求和符号</p><h2 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a><strong>小批量梯度下降</strong></h2><p>　　在了解了经典的梯度下降和随机梯度下降，并且知道其不同之处主要在于迭代过程中目标函数选择的不同。经典梯度下降虽然稳定性比较强，但是大样本情况下迭代速度较慢；随机梯度下降虽然每一步迭代计算较快，但是其稳定性不太好，而且实际使用中，参数的调整往往更加麻烦。<br>　　所以，为了协调稳定性和速度，小批量梯度下降应运而生。小批量梯度下降法和前面两种梯度下降的主要区别就是每一步迭代过程中目标函数的选择不同。小批量梯度下降是从 n个样本中随机且不重复地选择 m 个进行损失函数的求和</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080511.png" alt="image-20220929080511002">　　　</p><p>　　并将其作为每一步迭代过程中的目标函数。此时，迭代公式中的梯度也就变成了</p><p><img src="https://gitee.com/hexofox/foximage/raw/master/images/20220929080519.png" alt="image-20220929080519046">　　　　</p><p>　　显然，m=1 时，小批量梯度下降就是随机梯度下降，m=n 时，小批量梯度下降就是经典梯度下降。同时，我们也把经典的梯度下降方法称之为全批量梯度下降。这里的 m 一般称之为批量尺寸，其值的选择对于收敛的稳定性和速度有着较大的影响，也是一个技术活。<br>　　其他的也没什么好分析的了，基本上和随机梯度下降差不多。</p><hr><p>　　<strong>小批量梯度下降</strong>，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：<strong>每次迭代</strong> 使用 batch_size 个样本来对参数进行更新。</p><p>  这里我们假设batchsize=10，样本数m=1000</p><p>　<strong>优点：</strong><br>  （1）通过矩阵运算，每次在一个 batch 上优化神经网络参数并不会比单个数据慢太多。<br>  （2）每次使用一个 batch 可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置 batch_size=100 时，需要迭代 3000 次，远小于 SGD 的 30W 次)<br>  （3）可实现并行化。<br>  <strong>缺点：</strong><br>  （1）batch_size的不当选择可能会带来一些问题。<br>  <strong>batcha_size的选择带来的影响：</strong><br>  （1）在合理地范围内，增大batch_size的好处：<br>    a. 内存利用率提高了，大矩阵乘法的并行化效率提高。<br>    b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。<br>    c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。<br>  （2）盲目增大batch_size的坏处：<br>    a. 内存利用率提高了，但是内存容量可能撑不住了。<br>    b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。<br>    c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</p><p>博客链接：</p><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/BlairGrowing/p/15059613.html">机器学习——批量梯度下降法、随机梯度下降法、小批量梯度下降法 - 关注我更新论文解读 - 博客园 (cnblogs.com)</a></p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">FSRM</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://hexofox.gitee.io/2022/09/27/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/">https://hexofox.gitee.io/2022/09/27/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">FSRM</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">机器学习</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/2022/09/28/%E6%84%9F%E7%9F%A5%E6%9C%BA/"><div class="card-image"><img src="/medias/featureimages/22.jpg" class="responsive-img" alt="机器学习:感知机"> <span class="card-title">机器学习:感知机</span></div></a><div class="card-content article-content"><div class="summary block-with-text">机器学习:感知机1 感知机模型1.1 介绍感知机是神经网络（深度学习）的起源的算法同时也是“支持向量机”的基础。感知机接受多个输入信号，输出一个信号。 感知机(perceptron)是==二类分类==的==线性分类模型==，其输入为实例的特</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-09-28 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> FSRM</span></div></div><div class="card-action article-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">机器学习</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/2022/09/04/Dockerfile/"><div class="card-image"><img src="/medias/featureimages/24.jpg" class="responsive-img" alt="Dockerfile"> <span class="card-title">Dockerfile</span></div></a><div class="card-content article-content"><div class="summary block-with-text">简介 Dockerfile 是用来构建 Docker 镜像的文件，可以理解为命令参数脚本。 Dockerfile 是面向开发的，想要打包项目，就要编写 Dockerfile 文件。 官方Dockerfile 不过，官方镜像都是基础包，很多</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-09-04 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> FSRM</span></div></div><div class="card-action article-tags"><a href="/tags/Docker/"><span class="chip bg-color">Docker</span></a></div></div></div></div></article></div><script>$("#articleContent").on("copy",function(e){var n,t,o,i;void 0===window.getSelection||(""+(n=window.getSelection())).length<Number.parseInt("120")||(t=document.getElementsByTagName("body")[0],(o=document.createElement("div")).style.position="absolute",o.style.left="-99999px",t.appendChild(o),o.appendChild(n.getRangeAt(0).cloneContents()),"PRE"===n.getRangeAt(0).commonAncestorContainer.nodeName&&(o.innerHTML="<pre>"+o.innerHTML+"</pre>"),i=document.location.href,o.innerHTML+='<br />来源: FSRM<br />文章作者: FSRM<br />文章链接: <a href="'+i+'">'+i+"</a><br />本文章著作权归作者所有，任何形式的转载都请注明出处。",n.selectAllChildren(o),window.setTimeout(function(){t.removeChild(o)},200))})</script><script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{var t,n="prenext-posts";let e=$("#"+"artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#"+n).width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">FSRM</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">67k</span>&nbsp;字<br><br></div><div class="col s12 m4 l4 social-link"><a href="https://github.com/GhyJn0" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50"><i class="fas fa-rss"></i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/libs/materialize/materialize.min.js"></script><script src="/libs/masonry/masonry.pkgd.min.js"></script><script src="/libs/aos/aos.js"></script><script src="/libs/scrollprogress/scrollProgress.min.js"></script><script src="/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0],e=(t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",document.getElementsByTagName("script")[0]);e.parentNode.insertBefore(t,e)}()</script><script src="/libs/others/clicklove.js" async></script><script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async></script><script src="/libs/instantpage/instantpage.js" type="module"></script></body></html>