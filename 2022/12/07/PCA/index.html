<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="PCA:主成分析, java,安全,python"><meta name="description" content="一个小白成长史"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>PCA:主成分析 | FSRM</title><link rel="icon" type="image/png" href="/favicon.png"><link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="/libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="/css/matery.css"><link rel="stylesheet" type="text/css" href="/css/my.css"><script src="/libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="FSRM" type="application/atom+xml"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/" class="waves-effect waves-light"><img src="/medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">FSRM</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>归档</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/friends" class="waves-effect waves-light"><i class="fas fa-address-book" style="zoom:.6"></i> <span>友情链接</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">FSRM</div><div class="logo-desc">一个小白成长史</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="/about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="/contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="/friends" class="waves-effect waves-light"><i class="fa-fw fas fa-address-book"></i> 友情链接</a></li><li><div class="divider"></div></li><li><a href="https://github.com/ghyjn0" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i>Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/ghyjn0" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url(/medias/featureimages/7.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">PCA:主成分析</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="/libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{margin:35px 0 15px 0;padding-left:17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{height:calc(100vh - 250px);overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">机器学习</span></a></div></div><div class="col s5 right-align"></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2022-12-07</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp; 2022-12-10</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp; 1.8k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp; 6 分</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><h1 id="PCA-主成分析"><a href="#PCA-主成分析" class="headerlink" title="PCA:主成分析"></a>PCA:主成分析</h1><h2 id="1-PCA来源及作用"><a href="#1-PCA来源及作用" class="headerlink" title="1.PCA来源及作用"></a>1.PCA来源及作用</h2><h3 id="1-1-存在问题"><a href="#1-1-存在问题" class="headerlink" title="1.1 存在问题"></a>1.1 存在问题</h3><p>在我们训练模型的过程中，有时会出现在**<code>训练集上误差较小</code><strong>，但到了<code>测试集误差又较大</code>，我们称之为</strong><code>泛化误差</code>**，造成这种现象往往是以下几个原因：</p><ul><li>训练数据不足</li><li>训练集与测试集数据分布不同</li><li><strong><code>特征维度过高</code>，造成过拟合</strong></li></ul><h3 id="1-2-解决办法"><a href="#1-2-解决办法" class="headerlink" title="1.2 解决办法"></a>1.2 解决办法</h3><ul><li><p>增加样本数量</p></li><li><p>使用正则项</p></li><li><p>对数据进行降维对<code>数据进行降维</code>可以进行<code>人工特征筛选</code>，但往往费时又费力，效果还有可能不好。</p><p>因此我们可以采用一些<code>模型来进行数据降维</code>，其中比较常用的就是<code>PCA(Principal Component Analysis)，即主成分分析</code></p></li></ul><h2 id="2-什么是PCA"><a href="#2-什么是PCA" class="headerlink" title="2.什么是PCA"></a>2.什么是PCA</h2><p>主成分分析（PCA），<strong>一种无监督算法</strong>，即它<strong>不需要依靠任何类别标签的信息，</strong>是一种常用的<strong>线性降维方法</strong>。</p><p>该算法的目标是<strong>通过某种线性投影</strong>，将原本<strong>高维空间中的一些数据，映射到更低维度的空间中</strong>，并在所<strong>投影的维度上</strong>满足：<strong>1.尽可能保留原始数据的信息</strong>。<strong>2.新维度下变量间两两各不相关</strong>。</p><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210195843.png" alt="image-20221210195843175"></p><h2 id="3-PCA算法步骤"><a href="#3-PCA算法步骤" class="headerlink" title="3.PCA算法步骤"></a>3.PCA算法步骤</h2><ol><li><p><strong>对所有特征进行中心化：去均值(使得每一变量的均值为0)</strong></p></li><li><p><strong>计算协方差矩阵</strong></p></li><li><p><strong>计算协方差矩阵的特征值和特征向量</strong></p></li><li><p><strong>将特征值排序</strong></p></li><li><p><strong>保留前N个最大的特征值对应的特征向量</strong></p></li><li><p><strong>将原始特征转换到上面得到的N个特征向量构建的新空间中</strong></p></li></ol><p><strong><code>重点：5，6实现了特征压缩</code></strong></p><h2 id="4-PCA-例子"><a href="#4-PCA-例子" class="headerlink" title="4. PCA 例子"></a>4. PCA 例子</h2><blockquote><ol><li>数据示例</li></ol></blockquote><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210201742.png" alt="image-20221210201742170"></p><blockquote><ol start="2"><li>对所有特征进行中心化：去均值</li></ol></blockquote><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210201830.png" alt="image-20221210201830830"></p><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210193048.png" alt="image-20221210193048004"></p><blockquote><ol start="3"><li>求协方差矩阵C</li></ol></blockquote><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210202404.png" alt="image-20221210202404285"></p><p><code>C的维度是m ∗ m维的，m 为特征的数量。</code></p><p>　<strong>上述矩阵中，对角线上分别是特征x1和x2的方差，非对角线上的是协方差。协方差大于0，表示x1和x2正相关，小于0表示负相关，等于0，互相独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。</strong></p><p><strong><code>重点：</code></strong></p><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210202036.png"></p><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210202258.png" alt="image-20221210202258915"></p><p><code>解释：为什么除以n-1 而不是n:</code></p><p>这样能使我们<strong>以较小的样本集更好的逼近总体的标准差</strong>，即统计上<strong>所谓的“无偏估计”</strong></p><blockquote><ol start="4"><li>求协方差矩阵C的特征值和相对应的特征向量</li></ol></blockquote><p>利用线性代数的知识，求<code>协方差矩阵 C 的特征值 λ 和相对应的特征向量 u （每一个特征值对应一个特征向量</code>）公式如下:</p><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210203104.png" alt="image-20221210203104733"></p><p>将特征值 <strong>λ 按照从大到小的顺序排序，选择最大的前k个</strong>。</p><p>并将其相对应的k个特征向量拿出来，我们会得到一组{ ( λ 1 ， u 1 ) ， ( λ 2 ， u 2 ) ， . . . ， ( λ k ， u k ) } 。</p><p>本例中原始特征只有2维，我在选取 λ 的时候，令k = 1，选择最大的λ 1 和 其 对 应 的 u 1 。</p><p><strong>重点</strong>：<code>降低的维度k&lt;M k:所降低到的维度 M：特征数量个数</code></p><blockquote><ol start="5"><li>将原始特征投影到选取的特征向量上，得到降维后的新K维特征</li></ol></blockquote><p><strong>特征值从大到小排列</strong>，如果要将数据从<code>M维投影至k 维</code>，我们取<code>前k 个特征值对应的特征向量</code>，并且将其<code>标准化</code>，使得<code>每个特征向量的模为1</code>，将其从<code>上到下按行排列构成特征矩阵P .</code><br><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210204514.png" alt="image-20221210204514282"></p><p>投影后的的特征为Y**<code>（Y降维后的数据 ）</code>**：</p><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210204719.png" alt="image-20221210204719724"></p><blockquote><ol start="6"><li>最大熵理论</li></ol></blockquote><p>​ <strong>方差越大，信息量就越大</strong>。<strong>协方差矩阵的每一个特征向量就是一个投影面</strong>，<strong>每一个特征向量所对应的特征值就是原始特征投影到这个投影面之后的方差</strong>。<strong>由于投影过去之后，我们要尽可能保证信息不丢失，所以要选择具有较大方差的投影面对原始特征进行投影，也就是选择具有较大特征值的特征向量。然后将原始特征投影在这些特征向量上，投影后的值就是新的特征值。每一个投影面生成一个新的特征，k个投影面就生成k个新特征</strong>。</p><p>“**<code>协方差矩阵C的最大K个特征值所对应的特征向量”上的投影就是k维理想特征</code>**</p><p><code>那为什么协方差矩阵的特征向量可以看做是投影面，相对应的特征值是原始特征投影到这个投影面之后的方差？</code></p><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210205648.png" alt="image-20221210205648262"></p><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210205659.png" alt="image-20221210205659456"></p><p>根据方差最大理论，选择投影过去之后，样本方差最大的那条投影直线作为投影维度，对原始特征进行降维。经过计算，u 1 比较好，因为投影后的样本点之间方差最大</p><h2 id="5-PCA代码实现"><a href="#5-PCA代码实现" class="headerlink" title="5.PCA代码实现"></a>5.PCA代码实现</h2><blockquote><ol><li>实现PCA</li></ol></blockquote><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">class</span> <span class="token class-name">PCA</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 计算协方差矩阵</span>
    <span class="token keyword">def</span> <span class="token function">calc_cov</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token comment" spellcheck="true"># 数据标准化，X的每列减去列均值</span>
        X <span class="token operator">=</span> <span class="token punctuation">(</span>X <span class="token operator">-</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>X<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
        <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">.</span>T<span class="token punctuation">,</span> X<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">pca</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> n_components<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 计算协方差矩阵</span>
        cov_matrix <span class="token operator">=</span> self<span class="token punctuation">.</span>calc_cov<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 计算协方差矩阵的特征值和对应特征向量</span>
        eigenvalues<span class="token punctuation">,</span> eigenvectors <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>eig<span class="token punctuation">(</span>cov_matrix<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 对特征值排序x</span>
        idx <span class="token operator">=</span> eigenvalues<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment" spellcheck="true"># 取最大的前n_component组</span>
        eigenvectors <span class="token operator">=</span> eigenvectors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> idx<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#按特征值大小，从大到小排序</span>
        eigenvectors <span class="token operator">=</span> eigenvectors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>n_components<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#选取前 n_components 组特征向量</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"eigenvectors.shape = \n"</span><span class="token punctuation">,</span>eigenvectors<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"eigenvectors = \n"</span><span class="token punctuation">,</span>eigenvectors<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#一个特征向量是一列</span>
        <span class="token comment" spellcheck="true"># Y=PX转换</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> eigenvectors<span class="token punctuation">)</span>
</code></pre><blockquote><ol start="2"><li>导入数据</li></ol></blockquote><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token comment" spellcheck="true"># 导入sklearn数据集</span>
iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> iris<span class="token punctuation">.</span>data
y <span class="token operator">=</span> iris<span class="token punctuation">.</span>target
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

 <span class="token punctuation">(</span><span class="token number">150</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 150个样本 每个样本数据特征数为4</span>
</code></pre><blockquote><ol start="3"><li>PCA 降维并可视化</li></ol></blockquote><pre><code># 将数据降维到3个主成分
X_trans = PCA().pca(X, 3)
# 颜色列表
colors = [&#39;navy&#39;, &#39;turquoise&#39;, &#39;darkorange&#39;]
# 绘制不同类别
for c, i, target_name in zip(colors, [0,1,2], iris.target_names):
    plt.scatter(X_trans[y == i, 0], X_trans[y == i, 1],color=c, lw=1, label=target_name)
# 添加图例
plt.legend()
plt.show()
</code></pre><p><img src="https://gitee.com/hexofox/pic/raw/master/images/20221210210440.png" alt="image-20221210210440734"></p><h2 id="6-PCA-总结"><a href="#6-PCA-总结" class="headerlink" title="6. PCA 总结"></a>6. PCA 总结</h2><h3 id="6-1-优点"><a href="#6-1-优点" class="headerlink" title="6.1 优点"></a>6.1 优点</h3><ol><li>以方差衡量信息的无监督学习，不受样本标签限制。</li><li>由于协方差矩阵对称，因此k个特征向量之间两两正交，也就是各主成分之间正交，正交就肯定线性不相关，可消除原始数据成分间的相互影响</li><li>可减少指标选择的工作量</li><li>用少数指标代替多数指标，利用PCA降维是最常用的算法</li><li>计算方法简单，易于在计算机上实现。</li></ol><h3 id="6-2-缺点"><a href="#6-2-缺点" class="headerlink" title="6.2 缺点"></a>6.2 缺点</h3><ol><li>主成分解释其含义往往具有一定的模糊性，不如原始样本完整</li><li>贡献率小的主成分往往可能含有对样本差异的重要信息，也就是可能对于区分样本的类别（标签）更有用</li><li>特征值矩阵的正交向量空间是否唯一有待讨论</li></ol><h2 id="7-参考链接"><a href="#7-参考链接" class="headerlink" title="7.参考链接"></a>7.参考链接</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/BlairGrowing/p/15853516.html">chapter18——PCA实现 - 加微信X466550探讨 - 博客园 (cnblogs.com)</a></p><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/99kol/p/16693306.html">PCA原理及其代码实现 - 99号的格调 - 博客园 (cnblogs.com)</a></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1E5411E71z/?spm_id_from=333.337.search-card.all.click&vd_source=5e8f069711510b3788382a0a03ff38e5">用最直观的方式告诉你：什么是主成分分析PCA_哔哩哔哩_bilibili</a></p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="/about" rel="external nofollow noreferrer">FSRM</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="https://hexofox.gitee.io/2022/12/07/PCA/">https://hexofox.gitee.io/2022/12/07/PCA/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/about" target="_blank">FSRM</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">机器学习</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="/2023/01/11/Louvain/"><div class="card-image"><img src="/medias/featureimages/4.jpg" class="responsive-img" alt="Louvain算法"> <span class="card-title">Louvain算法</span></div></a><div class="card-content article-content"><div class="summary block-with-text">louvain算法1.社区1. 简介在最常见的社交网络中，每个用户相当一个点，用户之间的互相关注、点赞、私信等形成了边，用户以及相互作用关系构成了一个大的关系网络。在这样的网络中，有的用户之间的连接较为紧密，有的用户之间的连接关系较为稀疏。</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2023-01-11 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> FSRM</span></div></div><div class="card-action article-tags"><a href="/tags/%E7%A4%BE%E5%8C%BA%E5%88%92%E5%88%86/"><span class="chip bg-color">社区划分</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/2022/12/01/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"><div class="card-image"><img src="/medias/featureimages/0.jpg" class="responsive-img" alt="注意力机制"> <span class="card-title">注意力机制</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Attention(注意力机制)1.生物学背景视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2022-12-01 </span><span class="publish-author"><i class="fas fa-user fa-fw"></i> FSRM</span></div></div><div class="card-action article-tags"><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">深度学习</span></a></div></div></div></div></article></div><script>$("#articleContent").on("copy",function(e){var n,t,o,i;void 0===window.getSelection||(""+(n=window.getSelection())).length<Number.parseInt("120")||(t=document.getElementsByTagName("body")[0],(o=document.createElement("div")).style.position="absolute",o.style.left="-99999px",t.appendChild(o),o.appendChild(n.getRangeAt(0).cloneContents()),"PRE"===n.getRangeAt(0).commonAncestorContainer.nodeName&&(o.innerHTML="<pre>"+o.innerHTML+"</pre>"),i=document.location.href,o.innerHTML+='<br />来源: FSRM<br />文章作者: FSRM<br />文章链接: <a href="'+i+'">'+i+"</a><br />本文章著作权归作者所有，任何形式的转载都请注明出处。",n.selectAllChildren(o),window.setTimeout(function(){t.removeChild(o)},200))})</script><script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script><style type="text/css">code[class*=language-],pre[class*=language-]{white-space:pre!important}</style></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-",n=($("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)}),parseInt(.4*$(window).height()-64)),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),a=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),a.removeClass("l9")):(c.addClass(i).show(),a.addClass("l9"));{var t,n="prenext-posts";let e=$("#"+"artDetail");if(0!==e.length){let t=e.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#"+n).width(t)}return}})})</script></main><footer class="page-footer bg-color"><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2020</span> <a href="/about" target="_blank">FSRM</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">67k</span>&nbsp;字<br><br></div><div class="col s12 m4 l4 social-link"><a href="https://github.com/GhyJn0" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50"><i class="fas fa-rss"></i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script src="/js/search.js"></script><script type="text/javascript">$(function(){searchFunc("/search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/libs/materialize/materialize.min.js"></script><script src="/libs/masonry/masonry.pkgd.min.js"></script><script src="/libs/aos/aos.js"></script><script src="/libs/scrollprogress/scrollProgress.min.js"></script><script src="/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0],e=(t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",document.getElementsByTagName("script")[0]);e.parentNode.insertBefore(t,e)}()</script><script src="/libs/others/clicklove.js" async></script><script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async></script><script src="/libs/instantpage/instantpage.js" type="module"></script></body></html>